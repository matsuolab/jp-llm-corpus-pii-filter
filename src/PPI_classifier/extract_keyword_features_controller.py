# -*- coding: utf-8 -*-

import sys
import os
from pathlib import Path
from collections import Counter
import re
from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.base import BaseEstimator, TransformerMixin


### SRC
SRC_PATH = str(Path(__file__).resolve().parents[1])
# print(SRC_PATH)
sys.path.append(SRC_PATH)

class KeywordFeaturesExtractor(BaseEstimator, TransformerMixin):
    def __init__(self, keyword_list_file_paths: list, exist_flag=False) -> None:
        self.exist_flag = exist_flag    # True-> å‡ºç¾ã—ã¦ã„ã‚Œã°1, ãã†ã§ãªã‘ã‚Œã°0 , False -> å‡ºç¾é »åº¦
        self.NGkeywordDB = self.create_keywordDB(keyword_list_file_paths)

    def create_keywordDB(self, keyword_list_file_paths: list):
        """
        Returns:
            `dict`: {keyword: db_<filename>}
        """
        keywordDB = {}
        for path in keyword_list_file_paths:
            with open(path, 'r', encoding='utf-8') as f:
                _filename = path.split('/')[-1].split('.')[0]
                words = [w.strip() for w in f.readlines() if not len(w) == 0]
                for w in words:
                    if w not in keywordDB.keys():
                        keywordDB[w] = _filename
                    else:
                        print(f"Duplicate keyword: {w}")

        return keywordDB
    
    def extract_ng_keywords(self, text):
        keyword_count_list = []
        for word in self.NGkeywordDB.keys():
            if self.exist_flag is False:
                # å‡ºç¾é »åº¦
                keyword_count_list.append(text.count(word))
            else:
                # å‡ºç¾ã—ã¦ã„ã‚Œã°1, ãã†ã§ãªã‘ã‚Œã°0
                if word in text:
                    keyword_count_list.append(1)
                else:
                    keyword_count_list.append(0)
            
        return keyword_count_list
    
    def fit(self, X, y=None):
        """å­¦ç¿’ã¯ä¸è¦ã¨ã—ã¦selfã‚’è¿”ã™"""    
        return self
    
    def transform(self, X):
        return [self.extract_ng_keywords(text) for text in X]
    

class FullnameFeaturesExtractor(BaseEstimator, TransformerMixin):
    def __init__(self) -> None:
        self.lastname_dic, self.firstname_dict = self.create_name_dic()

    def create_name_dic(self):
        # äººåãƒªã‚¹ãƒˆã‚’è¿½åŠ 
        input_file = "/app/data/db/name_dic/unpack_jinmei30/JINMEI30_cutTop.TXT"
        lastname_dic = {}
        firstname_dict = {}
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’1è¡Œãšã¤èª­ã¿è¾¼ã‚€
        with open(input_file, "r", encoding="utf-8") as f:
            for line in f:
                m = re.match(r'(\S+)\t"(.*?)":(\S+)', line)
                if not m:
                    continue

                furigana, word, pos = m.group(1), m.group(2), m.group(3)
                if pos == "å§“":
                    lastname_dic[word] = furigana
                elif pos == "å":
                    firstname_dict[word] = furigana
        return lastname_dic, firstname_dict
    
    def count_first_and_last(self, text):
        """
        Returns:
            list: [count_lastname, count_firstname]
        """
        lastname_count = 0
        firstname_count = 0
        for w in self.lastname_dic.keys():
            lastname_count += text.count(w)
        for w in self.firstname_dict.keys():
            firstname_count += text.count(w)
        return [lastname_count, firstname_count]

    def fit(self, X, y=None):
        """å­¦ç¿’ã¯ä¸è¦ã¨ã—ã¦selfã‚’è¿”ã™"""    
        return self
    def transform(self, X):
        return [self.count_first_and_last(text) for text in X]

class SentenceContainTargetAndWord(BaseEstimator, TransformerMixin):
    def __init__(self, keyword_list_file_paths:list) -> None:
        """ TODO 
        percent"""
        self.NGkeywordDB = self.create_keywordDB(keyword_list_file_paths)
        self.lastname_dic, self.firstname_dict = self.create_name_dic()
    def create_keywordDB(self, keyword_list_file_paths: list):
        """
        Returns:
            `dict`: {keyword: db_<filename>}
        """
        keywordDB = {}
        for path in keyword_list_file_paths:
            with open(path, 'r', encoding='utf-8') as f:
                _filename = path.split('/')[-1].split('.')[0]
                words = [w.strip() for w in f.readlines() if not len(w) == 0]
                for w in words:
                    if w not in keywordDB.keys():
                        keywordDB[w] = _filename
                    else:
                        print(f"Duplicate keyword: {w}")

        return keywordDB
    def create_name_dic(self):
        # äººåãƒªã‚¹ãƒˆã‚’è¿½åŠ 
        input_file = "/app/data/db/name_dic/unpack_jinmei30/JINMEI30_cutTop.TXT"
        lastname_dic = {}
        firstname_dict = {}
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’1è¡Œãšã¤èª­ã¿è¾¼ã‚€
        with open(input_file, "r", encoding="utf-8") as f:
            for line in f:
                m = re.match(r'(\S+)\t"(.*?)":(\S+)', line)
                if not m:
                    continue

                furigana, word, pos = m.group(1), m.group(2), m.group(3)
                if pos == "å§“":
                    lastname_dic[word] = furigana
                elif pos == "å":
                    firstname_dict[word] = furigana
        return lastname_dic, firstname_dict
    
    def _check_contain_firstname(self, sentence:str):
        """ firstname(å)ãŒå«ã¾ã‚Œã‚‹ã‹ã©ã†ã‹"""
        for w in self.firstname_dict.keys():
            if w in sentence:
                return True
        return False

    def check_contain_last_or_first(self, sentence:str):
        """ lastname or firstnameãŒå«ã¾ã‚Œã‚‹ã‹ã©ã†ã‹ å«ã¾ã‚Œã¦ã„ã‚Œã°å³æ™‚return
        Returns:
            bool: True->å«ã¾ã‚Œã‚‹, False->å«ã¾ã‚Œãªã„
            is_lastname: True-> lastnameãŒå«ã¾ã‚Œã‚‹
        """
        for w in self.lastname_dic.keys():
            if w in sentence:
                return True, True
        if self._check_contain_firstname(sentence) is True:
            return True, False
        
        return False, False
    
    def check_contain_ng_word(self, sentence:str):
        """
        Returns:
            bool: True->å«ã¾ã‚Œã‚‹, False->å«ã¾ã‚Œãªã„
        """
        for w in self.NGkeywordDB.keys():
            if w in sentence:
                return True
        return False
    
    def count_target_and_ng_word(self, text):
        """ 1è¨˜äº‹ä¸­ã®å§“orå and NG_wordãŒå«ã¾ã‚Œã„ã¦ã„ã‚‹æ–‡ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        split_text = re.split(r"[ã€‚!?]\n?", text)
        split_text = [s.strip() for s in split_text if s]   # ç©ºã®è¦ç´ ã‚’å‰Šé™¤ï¼ˆæœ«å°¾ã«è¨˜å·ãŒã‚ã‚‹ã¨ç©ºæ–‡å­—ãŒã§ãã‚‹ãŸã‚ï¼‰
        first_or_last__NG_count = 0

        for s in split_text:
            # å§“ or åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ (è£œåŠ©æƒ…å ±: å§“ãªã‚‰ is_lastname=True)
            is_contian_target, is_lastname = self.check_contain_last_or_first(s)
            if is_contian_target is True:
                # å§“ or åã‚’å«ã‚€ -> check NG word
                is_contain_ng = self.check_contain_ng_word(s)
                if is_contain_ng is True:
                    first_or_last__NG_count += 1
        return first_or_last__NG_count, len(split_text)
    
    def count_match_sentence(self, text):
        """ 1è¨˜äº‹ä¸­ã®textã‚’å¥ç‚¹ã§ããã‚Šï¼Œå„æ–‡ã«å¯¾ã—ã¦ï¼Œtarget(äººåã‚’è¡¨ã™) and NG wordãŒå«ã¾ã‚Œã„ã¦ã„ã‚‹æ–‡ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        Returns:
            int:first_or_last__NG_count: (å§“orå) and NG_word ãŒå«ã¾ã‚Œã„ã¦ã„ã‚‹æ–‡ã®æ•°
            int: first_and_last__NG_count: (å§“andå) and NG_word ãŒå«ã¾ã‚Œã„ã¦ã„ã‚‹æ–‡ã®æ•°   ã“ã“ã§ã®å§“oråã¯å§“ï¼ŒåãŒå«ã‚“ã§ã„ã‚‹ã“ã¨ã‚’æ„å›³ã—ï¼Œé€£ç¶šã§ä¸¦ã‚“ã§ã„ã‚‹ã“ã¨ã¯è€ƒæ…®ã—ãªã„
            int: len(split_text): åˆ†å‰²ã—ãŸæ–‡ã®æ•° (æ”¹è¡Œ,ç©ºæ–‡å­—ã®ã¿ã®æ–‡ã¯é™¤å¤–)
        """
        split_text = re.split(r"[ã€‚!?]\n?", text)
        split_text = [s.strip() for s in split_text if s]   # ç©ºã®è¦ç´ ã‚’å‰Šé™¤ï¼ˆæœ«å°¾ã«è¨˜å·ãŒã‚ã‚‹ã¨ç©ºæ–‡å­—ãŒã§ãã‚‹ãŸã‚ï¼‰
        first_or_last__NG_count = 0
        first_and_last__NG_count = 0

        for s in split_text:
            # å§“ or åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ (è£œåŠ©æƒ…å ±: å§“ãªã‚‰ is_lastname=True)
            is_contian_target, is_lastname = self.check_contain_last_or_first(s)
            if is_contian_target is True:
                # å§“ or åã‚’å«ã‚€ -> check NG word
                is_contain_ng = self.check_contain_ng_word(s)
                if is_contain_ng is True:
                    first_or_last__NG_count += 1

                    # å§“ + NG -> åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                    if is_lastname is True:
                        if self._check_contain_firstname(s) is True:
                            first_and_last__NG_count += 1
    
        return first_or_last__NG_count, first_and_last__NG_count, len(split_text)
    
    def count_match(self, text):
        # default (å§“orå) and NG_word ã‚«ã‚¦ãƒ³ãƒˆæ•°
        # first_or_last__NG_count, sentence_num = self.count_target_and_ng_word(text)   # (å§“orå) and NG ã®ã‚«ã‚¦ãƒ³ãƒˆæ•°
        # return [first_or_last__NG_count]
    
        first_or_last__NG_count, first_and_last__NG_count, sentence_num = self.count_match_sentence(text)   # (å§“orå) and NG, (å§“,åä¸¡æ–¹å«ã‚€) amd NG ã®ã‚«ã‚¦ãƒ³ãƒˆæ•°
        if sentence_num == 0:
            return [0, 0, 0, 0]
        return [first_or_last__NG_count, first_and_last__NG_count, (first_or_last__NG_count/sentence_num)*100, (first_and_last__NG_count/sentence_num)*100]
        
    
    def fit(self, X, y=None):
        """å­¦ç¿’ã¯ä¸è¦ã¨ã—ã¦selfã‚’è¿”ã™"""    
        return self
    def transform(self, X):
        return [self.count_match(text) for text in X]



def union_features():
    # Model
    ppi_ng_dic_files = ['/app/data/db/medical_history_ja_202410.txt',
                        '/app/data/db/criminal_history_ja_202410.txt',
                        '/app/data/db/religion_ja_202412.txt',
                        '/app/data/db/religion_believer_noun_ja_202412.txt',
                        '/app/data/db/religion_tuushou_unique_ja_202412.txt',
                        '/app/data/db/race_ethnic_generation_ja_202412.txt',
    ]
    from sklearn.naive_bayes import MultinomialNB

    pipeline = Pipeline([
        ('features', FeatureUnion([
            ('keywords', KeywordFeaturesExtractor(ppi_ng_dic_files)),
            # ('ngram_count', NgramCountVectorizer(ngram_range=(1, 2))),
            # ('ngram_tfidf', NgramTfidfVectorizer(ngram_range=(1, 2))),
            ])),
        # ('classifier', MultinomialNB())
    ])

    samples = ["ã“ã®å‰ã€å‹é”ã¨è¡Œã£ãŸãƒ©ã‚¤ãƒ–ä¼šå ´ã§ã€ã‚ã£ã¡ã‚ƒæœ‰åãªã‚®ã‚¿ãƒªã‚¹ãƒˆã®ä½è—¤å¥å¤ªã•ã‚“ã‚’è¦‹ã‹ã‘ãŸã‚“ã ã‘ã©ã€ãªã‚“ã‹ã€ãƒ©ã‚¤ãƒ–ãŒçµ‚ã‚ã£ãŸå¾Œã«ã€ã™ã”ã„è½ã¡è¾¼ã‚“ã§ãŸã‚“ã ã‚ˆã­ã€‚ã—ã‹ã‚‚ã€ãªã‚“ã‹ã€ãƒˆã‚¤ãƒ¬ã®å‰ã§ã€ãšã£ã¨ã†ãšãã¾ã£ã¦ã¦ã€æ§˜å­ãŒãŠã‹ã—ã‹ã£ãŸã€‚ã‚‚ã—ã‹ã—ãŸã‚‰ã€ä½•ã‹ã‚ã£ãŸã®ã‹ãªã£ã¦æ€ã£ã¦ã€ã¡ã‚‡ã£ã¨å¿ƒé…ã«ãªã£ãŸã€‚ä½è—¤å¥å¤ªã•ã‚“ã£ã¦ã€ã„ã¤ã‚‚ã€ã‚ã‚“ãªã«å ‚ã€…ã¨ã—ã¦ã‚‹ã®ã«ã€å®Ÿã¯ã€ç¹Šç´°ãªäººãªã®ã‹ã‚‚ã—ã‚Œãªã„ã£ã¦æ€ã£ãŸã€‚ãã®å¾Œã€ä½è—¤ã•ã‚“ã®SNSã¨ã‹ã‚‚ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ãŸã‘ã©ã€ç‰¹ã«ä½•ã‚‚æ›¸ã„ã¦ãªãã¦ã€ä½™è¨ˆã«å¿ƒé…ã«ãªã£ãŸã€‚ã‚‚ã—ã‹ã—ãŸã‚‰ã€ä½“èª¿ãŒæ‚ªã‹ã£ãŸã®ã‹ã‚‚ï¼Ÿãã‚Œã¨ã‚‚ã€ä½•ã‹ç²¾ç¥çš„ã«å‚ã£ã¦ã‚‹ã®ã‹ãªï¼Ÿã¨ã«ã‹ãã€æ—©ãå…ƒæ°—ã«ãªã£ã¦ã»ã—ã„ãªã€‚",
               "Facebookã§ã¡ã‚‡ã£ã¨ã—ãŸé¨’ãã«ãªã£ã¦ãŸè©±ã€ã‚ã®å±±ç”°å½©éŸ³ã•ã‚“ãŒã€è‹¥ã„é ƒã«ä¸€åº¦çµå©šã—ã¦ãã®å¾Œé›¢å©šã—ãŸã“ã¨ãŒã‚ã‚‹ã£ã¦çŸ¥ã£ã¦ãŸï¼ŸğŸ˜² å‘¨ã‚Šã®äººãŸã¡ã¯å½¼å¥³ã®ä»Šã®é­…åŠ›çš„ãªã‚­ãƒ£ãƒªã‚¢ã«ã—ã‹æ³¨ç›®ã—ãªã„ã‹ã‚‰ã€ãã‚“ãªéå»ãŒã‚ã‚‹ãªã‚“ã¦ã³ã£ãã‚Šã ã‚ˆã€‚ã§ã‚‚ã€é›¢å©šã£ã¦ã™ã”ããƒ‘ãƒ¼ã‚½ãƒŠãƒ«ãªã“ã¨ã ã‚ˆã­ã€‚ä»–ã®äººã«ã¯çŸ¥ã‚‰ã‚Œãªãã¦ã‚‚ã„ã„ã‚ˆã†ãªæƒ…å ±ãªã®ã«ã€ãƒãƒƒãƒˆã§å›ã£ã¡ã‚ƒã†ã®ã‚’ç›®ã®å½“ãŸã‚Šã«ã™ã‚‹ã¨ã€ã‚„ã£ã±ã‚Šã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ€–ã„ãªã‚ã£ã¦æ€ã†ã‚ˆã€‚",
               "ã†ã‚ããƒã‚¸ã‹ã‚ˆâ€¦éš£ã«ä½ã‚“ã§ã‚‹ä¸­æ‘ã•ã‚“ãŒçµ±åˆå¤±èª¿ç—‡ã§é€šé™¢ã—ã¦ã‚‹ã£ã¦çŸ¥ã£ã¦è¡æ’ƒãªã‚“ã ã‘ã©ã€‚ã„ã¤ã‚‚ãƒ‹ã‚³ãƒ‹ã‚³ã—ã¦ã¦å„ªã—ã„äººãªã®ã«ã€‚æœ€è¿‘æ§˜å­ãŒãŠã‹ã—ã‹ã£ãŸã¨æ€ã£ãŸã‚‰ã€è–¬ã®å‰¯ä½œç”¨ã§ä½“èª¿å´©ã—ã¦ãŸã‚‰ã—ã„ã€‚å®¶æ—ã‚‚å¤§å¤‰ãã†ã€‚orz ã§ã‚‚é ‘å¼µã£ã¦æ²»ç™‚ç¶šã‘ã¦ã‚‹ã¿ãŸã„ã€‚ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã«åˆ‡ã‚Šæ›¿ãˆãŸã®ã‚‚ãã®ã›ã„ã ã£ãŸã®ã­ã€‚èª°ã«ã‚‚è¨€ãˆãªã„ç§˜å¯†ã ã‘ã©ã€å¿œæ´ã—ãŸã„(ï½€ãƒ»Ï‰ãƒ»Â´)ï¾‰"]
    prepared = pipeline.fit_transform(samples)
    print(prepared)
    print(" ".join([str(x) for x in prepared[2]]))

if __name__ == '__main__':
    
    ### Test
    # test_keyword()
    # test_ngram_tfidf()
    union_features()